{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPS242 - Homework 3\n",
    "Yi Ru 1635532, Xiaotong Li 1634362  \n",
    "October 22, 2017  \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "In this homework report, we explored and applied the logistic regression and the gradient descent theory, to realize a spam email detector. In section 2, we will introduce some basic mathematical and programming methods used in this homework. In section 3, we will introduce the whole algorithm and realization of the detector, and some relative theories and amendments to the detector, as well as their comparisons with the Batch Gradient Descent. In section 4, we will draw our conclusions. \n",
    "\n",
    "\n",
    "\n",
    "## 2. Theory\n",
    "\n",
    "\n",
    "### 2.1 Logistic Regression\n",
    "In this case of spam email recognition, we need to apply certain theory in machine learning to classify the data we got. However, the linear regression model we used previously can not guarantee valid classification when the data amount is so large. So we need to introduce the logistic regression model to solve this, whose mathematical form is as follows: \n",
    "$$ h(z)=\\frac{1}{1+e^{-z}}\\tag{2.1}$$\n",
    "Through applying this function, we can map a linear distribution $z$ to a probability distribution where $h(z)$ is bounded by 0 and 1. So we can use this model to classify the data validly as we want to. \n",
    "\n",
    "\n",
    "### 2.2 Cost Function\n",
    "To evaluate the level of the classification resulted from parameter fitting, we introduce a cost function using cross entropy:\n",
    "$$ J(\\omega)=\\frac{1}{M}\\sum_{i=1}^{M} \\left( y_i \\text{log}\\hat{y_i} + (1-y_i) \\text{log}(1-\\hat{y_i}) \\right)\n",
    "\\tag{2.2}$$\n",
    "Where $J(\\omega)$ represents the cost of the whole model, $\\omega$ represents the current parameter, $M$ represents the total number of the data sets, and $y_i$ represents the true value while $\\hat{y_i}$ represents the estimated value.  \n",
    "The lower the cost function of a model is, the more effective the classification of data is. So how to find the parameter to make the cost function reach its minimum is the problem worth solving.\n",
    "\n",
    "\n",
    "### 2.3 Gradient Descent\n",
    "To achieve the parameter which can make the cost function reaches its minimum, we need to introduce the gradient descent method to approach the minimum value of a function. More specifically, there are three different methods, each of which has its own form of recursive function as follows: $\\quad$  \n",
    "(1) Batch Gradient Descent  \n",
    "The basic algorithm of BGD is as follows[1]:  \n",
    "> Repeat until convergence$\\{$ $$\\omega_j:=\\omega_j+\\alpha\\sum_{i=1}^{m}\\left(y^{(i)}-h_{\\omega}(x^{(i)})\\right)x_j^{(i)}\\quad\\text{for every }j.$$\n",
    "$\\}$   \n",
    "\n",
    "When the above equation converges to certain amount, that is to say the two consecutive iterations don't have a difference bigger than convergence, it quits the recursive loop. $\\alpha$ is the learning rate, which determines the step of the descent.  \n",
    "What \"Batch\" means here is that the output of each parameter $\\theta$ within the parameter set $\\omega$ should be calculated only when all the training data are covered.  \n",
    "$\\quad$  \n",
    "(2) Stochastic Gradient Descent  \n",
    "The basic algorithm of SGD is as follows[1]:   \n",
    "> Loop$\\{$  \n",
    "$\\quad$for $i=1$ to $m$ ,$\\{$\n",
    "$$\\omega_j:=\\omega_j+\\alpha\\sum_{i=1}^{m}\\left(y^{(i)}-h_{\\omega}(x^{(i)})\\right)x_j^{(i)}\\quad\\text{for every }j.$$\n",
    "$\\quad\\}$  \n",
    "$\\}$  \n",
    "\n",
    "This method only calculated the parameters by considering the current training data, regardless of the other training data point, which will make the algorithm much more efficient. However, due to the inconsideration of other points, the recursive process will be quite turtuous, and most of time it can only approach the local best instead of reaching it. So it suits for the case where there are too many samples.  \n",
    "$\\quad$   \n",
    "\n",
    "### 2.4 Text Feature Extraction\n",
    "Since the object of our homework is to identify and classify emails, we need to extract the features of the text in the emails and evaluate the weights of each word. Thus we introduce the TF-IDF (term frequencyâ€“inverse document frequency) transform to process the email text provided, the basic calculation of which is as follows[2]: \n",
    "$$tf_{i,j}=\\frac{n_{i,j}}{\\sum_k n_{k,j}}\\tag{2.7}$$\n",
    "$$idf_i=\\text{log} \\frac{\\left| D \\right|}{\\left |\\{j:t_i \\in d_j\\}\\right|}\\tag{2.4}$$\n",
    "$$tfidf_{i,j}= tf_{i,j} \\times idf_i \\tag{2.8}$$\n",
    "Where $ n_{i,j}$ represents the times that term $t_i$ emerges in the file $d_j$ , while $\\left| D \\right|$ represents the total amount of the files in the database, and $\\left |\\{j:t_i \\in d_j\\}\\right|$ represents the amount of the files that contain term $t_i$ .\n",
    "So the tdidf value is the tool that we use to evaluate the importance of a certain term within the emails database, which can be used to judge whether a email is a spam or not. \n",
    " \n",
    " \n",
    "### 2.5 Regularization & 10-fold Cross Validation\n",
    "To avoid over-fitting in the parameter calculation, we need to introduce a regularizer to amend the cost function. In most cases, we use $\\lambda ||\\omega||^2$ as the regularizer, which will make the cost function to be as follows[1]: \n",
    "$$ J(\\omega)=\\frac{1}{M}\\sum_{i=1}^{M} \\left( y_i \\text{log}\\hat{y_i} + (1-y_i) \\text{log}(1-\\hat{y_i}) \\right)+ \\frac{\\lambda}{M} ||\\omega||^2\n",
    "\\tag{2.9}$$\n",
    "To get the optimum value of the regularization parameter $\\lambda$ , we need to introduce the 10-fold cross validation method, which is the same as the first homework.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Spam Email Detector Realization\n",
    "\n",
    "\n",
    "### 3.1 Batch Gradient Descent (Original Model)\n",
    "The original basic method we designed to realize the spam email detector is as follows:    \n",
    "$\\quad$  \n",
    "Text preprocess (Tokenize & Remove the stopwords and punctuations)  \n",
    "Calculate and get the $x=\\text{tf-idf}$ matrix  \n",
    "For $\\lambda = a$ to $b$  \n",
    "$\\quad$For 10-fold cross validation  \n",
    "$\\quad \\quad$While cost function$>\\epsilon$  \n",
    "$\\quad \\quad \\quad$For $i=1$ to $N$   \n",
    "$\\quad \\quad \\quad \\quad$Update $\\theta_i$ within parameter set $\\omega$  \n",
    "$\\quad \\quad \\quad \\quad$For $t=1$ to $M$  \n",
    "$\\quad \\quad \\quad \\quad \\quad$Calculate $\\hat{y_i}$ and cost  \n",
    "$\\quad \\quad \\quad \\quad$End  \n",
    "$\\quad \\quad \\quad$End  \n",
    "$\\quad$End  \n",
    "Calculate the average loss of the 10 folds  \n",
    "End  \n",
    "Find the best $\\lambda$  \n",
    "$\\quad$  \n",
    "Where $N$ represents the total number of the valid terms within text data, while $M$ represents the total number of the training email amount each time.\n",
    "\n",
    "#### 3.1.1 Simulation\n",
    "The libraries we used to realize the whole function of the detector is as follows (The same to the next few amendments): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "from numpy import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to realize the text preprocess (Tokenize, Remove the stopwords and punctuations, Get the tf-idf matrix) function is as follows (The same to the next few amendments): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "v = TfidfVectorizer(smooth_idf=False)\n",
    "stop = stopwords.words('english')\n",
    "sent_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# first part of homework3\n",
    "my_list = []\n",
    "csv_file = 'train.csv'\n",
    "Processed_csv = 'new_train.csv'\n",
    "new_file = open(Processed_csv , 'w')\n",
    "fr = csv.writer(new_file, dialect=\"excel\")\n",
    "\n",
    "# we need to process the data in the train.csv first\n",
    "\n",
    "\n",
    "\n",
    "def process(mail,label):\n",
    "    if label == \"ham\" :\n",
    "        new_label = 1\n",
    "    else :\n",
    "        new_label = 0\n",
    "\n",
    "    mail = mail.lower()\n",
    "    mail_tokenize = sent_tokenizer.tokenize(mail)\n",
    "    filtered_words = [' '.join(w for w in mail_tokenize if not w in stop)]\n",
    "    filtered_words.insert(0,new_label)\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "with open(csv_file, 'r', encoding='latin-1') as csv_reader:\n",
    "    next(csv_reader)\n",
    "    # we use csv_reader function to read documents from train.csv\n",
    "    reader = csv.reader(csv_reader)\n",
    "    my_list = list(reader)\n",
    "    # we create two variables and save the content of mail into variable mail\n",
    "    for row in my_list:\n",
    "        mail = row[1]\n",
    "        label = row[0]\n",
    "        fr.writerow(process(mail,label))\n",
    "new_file.close()\n",
    "with open(Processed_csv, 'r', encoding='latin-1') as new_reader:\n",
    "    new_train = csv.reader(new_reader)\n",
    "    words = []\n",
    "    for lines in new_train:\n",
    "        words.append(lines[1])\n",
    "words_Frequency = vectorizer.fit_transform(words)\n",
    "words_bag = vectorizer.get_feature_names()\n",
    "tf_idf = transformer.fit_transform(words_Frequency)\n",
    "\n",
    "# normalize the tf-idf matrix\n",
    "# get the maximum and minimum value\n",
    "tf_idf_min, tf_idf_max = tf_idf.min(), tf_idf.max()\n",
    "# (matrix elements-minimum)/(maximum-minimum)\n",
    "a = (tf_idf - tf_idf_min) / (tf_idf_max - tf_idf_min)\n",
    "words = []\n",
    "labels = []\n",
    "with open('new_train.csv', 'r') as new_csv_reader:\n",
    "    reader = csv.reader(new_csv_reader)\n",
    "    for line in reader:\n",
    "        if line:\n",
    "            words.append(line[1])\n",
    "            labels += line[0]\n",
    "\n",
    "error_sum = 0\n",
    "lambda_value = []\n",
    "y = numpy.matrix(labels).transpose().astype(int)\n",
    "w = numpy.zeros((tf_idf.shape[1],1))\n",
    "cross_validation = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to realize the whole function of the detector using Batch Gradient Descent is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGradient:\n",
    "    def batch_gd(x, y, w, lam):\n",
    "        w = numpy.zeros((x.shape[1], 1))      # w is matrix 6125 x 1 as temporary w\n",
    "        error_sum = 0\n",
    "        error0=0\n",
    "        n = 1\n",
    "        while True:\n",
    "            z = (numpy.matmul(-x, w))  # matrix 2700 x 1   [2700x6125] x [6125x1]=2700x1\n",
    "            # now set  n0 = 0.3\n",
    "            learning_rate = 0.3 * (n ** (-0.9))\n",
    "            for i in range(len(z)):  # i from (0,2700)\n",
    "                # print(\"now is selecting   \" + str(i) + \" e-mail\")\n",
    "                y_tutor = 1 / (1 + math.exp(z[i]))\n",
    "                dif = numpy.matrix((y_tutor - y[i][0]) * x[i, :]).transpose()\n",
    "                error_sum += y[i] * math.log2(y_tutor) + (1 - y[i]) * math.log2(1 - y_tutor)\n",
    "                for j in range(len(w)):  # j from (0,6125), done one e-mail than update one w\n",
    "                    w[j][0] = w[j][0] - learning_rate * error_sum\n",
    "            error = (1 / len(z)) * (error_sum + lam * numpy.matmul(w.transpose(), w))\n",
    "            if abs(error-error0) < 0.1:\n",
    "                break\n",
    "            else:\n",
    "                error0 = error\n",
    "                w = w\n",
    "                n+=1\n",
    "                print(w)\n",
    "        print(\"Done ,the best w is \", w)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Result\n",
    "The result of the above method is as follows:   \n",
    "Error:11%\n",
    "  (0, 0)\t0.971364036086  \n",
    "  (0, 1)\t0.759948280032  \n",
    "  (0, 2)\t0.95585104881  \n",
    "  (0, 3)\t0.971364036086  \n",
    "  (0, 4)\t0.971364036086  \n",
    "  (0, 5)\t0.0599466459071  \n",
    "  (0, 6)\t0.971364036086  \n",
    "  (0, 7)\t0.971364036086  \n",
    "  (0, 8)\t0.971364036086  \n",
    "  (0, 9)\t0.971364036086  \n",
    "  (0, 10)\t0.971364036086  \n",
    "  (0, 11)\t0.971364036086  \n",
    "  (0, 12)\t0.0137686298225  \n",
    "  (0, 13)\t0.791249357017  \n",
    "  (0, 14)\t0.971364036086  \n",
    "  (0, 15)\t0.971364036086  \n",
    "  (0, 16)\t0.623288242111  \n",
    "  (0, 17)\t0.971364036086  \n",
    "  (0, 18)\t0.971364036086  \n",
    "  (0, 19)\t0.971364036086   \n",
    "  (0, 20)\t0.0279610823561   \n",
    "  (0, 21)\t0.198062010111  \n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  (0, 6144)\t0.971364036086  \n",
    "Computation Time: 4 hours \n",
    "  \n",
    "### 3.2 Stochastic Gradient Descent \n",
    "\n",
    "#### 3.2.1 Simulation\n",
    "The code to realize the whole function of the detector using Stochastic Gradient Descent is as follows:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGradient:\n",
    "    def stochastic_gd(tf_idf,y,w,lambda_1):\n",
    "        # i don't know what the value of the epsilon\n",
    "        # initial value\n",
    "        w_value = numpy.zeros(tf_idf.shape[1],1)\n",
    "        error_1 = 0\n",
    "        error_0 = 0\n",
    "        n = 1 # count number\n",
    "        while True:\n",
    "            h_w = (numpy.matmul(-tf_idf.toarray(),w))\n",
    "            learning_rate = 0.3*(n ** (-0.9))\n",
    "            print(\"this is the \", n , \"times studying\")\n",
    "            print(\"=================================\")\n",
    "            for i in range(len(h_w))ï¼š\n",
    "                y_tutor = 1 / (1+math.exp(h_w[i]))\n",
    "                diverg = numpy.matrix((y_tutor-y[i][0]) * tf_idf[i, :]).transpose()\n",
    "                for j in range(len(w_value)):\n",
    "                    w_value[j][0] = w_value[j][0] - learning_rate * diverg[j][0]\n",
    "            error = (1 / len(h_w)) * (error_1 + lambda_1 * numpy.matmul(w_value.transpose(), w_value))\n",
    "            if abs(error-error_0) < 0.1:\n",
    "                break\n",
    "            else:\n",
    "                error_0 = error\n",
    "                w = w_value\n",
    "                n +=1\n",
    "                print(w)\n",
    "        print(\"studying is over and the best w is \", w)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Result & Comparision with BG\n",
    "The stochastic gradient descent is more faster than BG, but it is more easier coverge at local optimal solution. \n",
    "  \n",
    "  \n",
    "### 3.3 Exponentiated Gradient Descent\n",
    "\n",
    "#### 3.3.1 Simulation\n",
    "The code to realize the the whole function of the detector using Exponentiated Gradient Descent is as follows[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExponentialGradient:\n",
    "    def test_case_1(self):\n",
    "        data_set = a.toarray()\n",
    "        data_set_1 = numpy.array(data_set)\n",
    "        self.weighted_vote_method(data_set_1)\n",
    "\n",
    "    def weighted_vote_method(self, data_set, learning_rate = 0.3):\n",
    "        if len(data_set) <= 0:\n",
    "            raise ValueError(\"Data set length error.\")\n",
    "        weight_result =  []\n",
    "        current_weight = [1. / len(data_set[0,:]) for i in range(len(data_set[0,:]))]\n",
    "        for i in range(len(data_set[0])):\n",
    "            #print(\"Current weight=\\t\\t\" + str(current_weight))\n",
    "            current_weight_plus = current_weight\n",
    "            current_weight_minus = current_weight\n",
    "            current_weight = self.exponentiated_gradient(data_set[i,:], current_weight_plus, current_weight_minus, learning_rate)\n",
    "            weight_result.append(current_weight)\n",
    "            #print(\"===================\")\n",
    "    def exponentiated_gradient(self, data_set, previous_weight_plus, previous_weight_minus, learning_rate):\n",
    "        if len(data_set) <= 0:\n",
    "            raise ValueError(\"Data set length error.\")\n",
    "        if len(data_set) != len((previous_weight_plus-previous_weight_minus):\n",
    "            raise ValueError(\"Arguments length not equal.\")\n",
    "\n",
    "        #print(\"Data set =\\t\\t\" + str(data_set))\n",
    "\n",
    "        result = []\n",
    "        all_weighted_value = numpy.sum([previous_weight[i] * data_set[i] for i in range(len(data_set))])\n",
    "        # update the w+ and w-\n",
    "        numerator_plus = numpy.sum([previous_weight_plus[i] * numpy.exp((learning_rate * data_set[i]) / all_weighted_value) for i in range(len(data_set))])\n",
    "        numerator_minus = numpy.sum([previous_weight_minus[i] * numpy.exp(-(learning_rate * data_set[i]) / all_weighted_value) for i in range(len(data_set))])\n",
    "        #print(\"Numerator=\\t\\t\\t\" + str(numerator))\n",
    "\n",
    "        for i in range(len(data_set)):\n",
    "            fractions = (previous_weight_plus[i]-previous_weight_minus[i]) * numpy.exp((learning_rate * data_set[i]) / all_weighted_value)\n",
    "            result.append(fractions / (numerator_plus-numerator_minus)\n",
    "        #print(\"Result=\\t\\t\\t\\t\" + str(result))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Result & Comparision with BG\n",
    "The result of the above method is as follows:  \n",
    "Final Cost:  \n",
    "Computation Time:  \n",
    "  \n",
    "\n",
    "### 3.4 Additional Work\n",
    "\n",
    "We change the regularizer in the function (2.9) to $\\lambda||\\omega||$ . And we can get the following results:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGradient:\n",
    "    def batch_gd(x, y, w, l):\n",
    "        w = numpy.zeros((x.shape[1], 1))      # w is matrix 6125 x 1 as temporary w\n",
    "        error_sum = 0\n",
    "        error0=0\n",
    "        n = 1\n",
    "        while True:\n",
    "            z = (numpy.matmul(-x, w))  \n",
    "            # now set  n0 = 0.3\n",
    "            learning_rate = 0.3 * (n ** (-0.9))\n",
    "            for i in range(len(z)):  # i from (0,2700)\n",
    "                y_tutor = 1 / (1 + math.exp(z[i]))\n",
    "                dif = numpy.matrix((y_tutor - y[i][0]) * x[i, :]).transpose()\n",
    "                error_sum += y[i] * math.log2(y_tutor) + (1 - y[i]) * math.log2(1 - y_tutor)\n",
    "                for j in range(len(w)):  # j from (0,6125), done one e-mail than update one w\n",
    "                    w[j][0] = w[j][0] - learning_rate * dif[j][0]\n",
    "            error = (1 / len(z)) * (error_sum + l * numpy.matmul(w.transpose(), w))\n",
    "            if abs(error-error0) < 0.1:\n",
    "                break\n",
    "            else:\n",
    "                error0 = error\n",
    "                w = w\n",
    "                n+=1\n",
    "                print(w)\n",
    "        print(\"Done ,the best w is \", w)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this homework, we use logistic regression as tools to handle the problem of spam email detection. The method we use have batch gradient descent, stochastic gradient descent and exponential gradient descent. The simulation results show that the logistic regression can effectively classify the spam email and ham email, but different methods have different computation time and convergence speed. In general, because the batch gradient descent need to cover all data in train set so it is cost large time on computation and convergence speed.With the regularization term $\\lambda$, we can effectively avoid the problem of overfitting and we try different regularization term like $\\lambda ||\\omega||$, but because of time problem, we can not try every method like weighted least linear square and comparsion.\n",
    "\n",
    "## 5.Reference\n",
    "1. http://www.cnblogs.com/rcfeng/p/3958926.html \n",
    "2. http://blog.csdn.net/sangyongjia/article/details/52440063\n",
    "3. Kivinen, Jyrki, and Manfred K. Warmuth. \"Exponentiated gradient versus gradient descent for linear predictors.\" Information and Computation 132.1 (1997): 1-63."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
